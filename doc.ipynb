{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Advance Embedding Techniques**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Importing Required Libraries and Setting Up Environment Variables First, let's import the necessary libraries and load the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "import pickle\n",
    "import zlib\n",
    "import logging\n",
    "import faiss\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from redis import StrictRedis\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initialize Flask, MongoDB, Redis, and Set Up Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Flask app and enable CORS for all routes\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables for model configurations\n",
    "ollama_model = os.getenv(\"OLLAMA_MODEL\")\n",
    "embed_model = os.getenv(\"OLLAMA_EMBED_MODEL\")\n",
    "mongo_uri = os.getenv(\"MONGO_URI\")\n",
    "redis_host = os.getenv(\"REDIS_HOST\", \"localhost\")\n",
    "redis_port = os.getenv(\"REDIS_PORT\")\n",
    "\n",
    "# Set up MongoDB client and collections\n",
    "client = MongoClient(mongo_uri)\n",
    "db = client[\"Embeddings\"]\n",
    "collection = db[\"ksm_compress_embeddings\"]\n",
    "chat_history_collection = db[\"ksm_chat_history\"]\n",
    "\n",
    "# Set up Redis client\n",
    "cache = StrictRedis(host=redis_host, port=redis_port, db=0)\n",
    "\n",
    "# Use Ollama for embeddings\n",
    "embeddings_model = OllamaEmbeddings(model=embed_model)\n",
    "\n",
    "# Memory setup for chat history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define Utility Functions\n",
    "    Text Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_files(file_paths):\n",
    "    \"\"\"Function to extract text from PDF files.\"\"\"\n",
    "    text_contents = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            pdf_reader = PdfReader(file)\n",
    "\n",
    "            # Extract text from each page\n",
    "            for page in pdf_reader.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:  # Only append non-empty text\n",
    "                    text_contents.append(text)\n",
    "\n",
    "    return text_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Text Splitting Function```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Split text into chunks for embedding.\"\"\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Compression and Decompression Functions```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compress data\n",
    "def compress_data(data):\n",
    "    \"\"\"Compress data using zlib.\"\"\"\n",
    "    return zlib.compress(data)\n",
    "\n",
    "\n",
    "# Function to decompress data\n",
    "def decompress_data(compressed_data):\n",
    "    \"\"\"Decompress data using zlib.\"\"\"\n",
    "    return zlib.decompress(compressed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create or Load FAISS Vector Store\n",
    "This function creates or loads a FAISS index from MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_or_load_vectorstore(text_chunks, index_type=\"HNSW\"):\n",
    "    \"\"\"Create or load the FAISS vector store from MongoDB.\"\"\"\n",
    "    index = None\n",
    "    try:\n",
    "        # Step 1: Check if FAISS index exists in MongoDB\n",
    "        if collection.count_documents({}) > 0:\n",
    "            print(\"Loading FAISS vector store from MongoDB...\")\n",
    "\n",
    "            # Step 2: Retrieve FAISS data from MongoDB\n",
    "            embeddings = collection.find_one({})\n",
    "            faiss_data = embeddings[\"faiss_index\"]\n",
    "            docstore_data = embeddings.get(\"docstore\")\n",
    "            index_to_docstore_id_data = embeddings.get(\"index_to_docstore_id\")\n",
    "\n",
    "            # Step 3: Deserialize and decompress FAISS index and docstore\n",
    "            faiss_index = pickle.loads(decompress_data(faiss_data))\n",
    "            docstore = pickle.loads(decompress_data(docstore_data))\n",
    "            index_to_docstore_id = pickle.loads(\n",
    "                decompress_data(index_to_docstore_id_data)\n",
    "            )\n",
    "\n",
    "            # Step 4: Create FAISS vector store from deserialized data\n",
    "            vectorstore = FAISS(\n",
    "                index=faiss_index,\n",
    "                docstore=docstore,\n",
    "                index_to_docstore_id=index_to_docstore_id,\n",
    "                embedding_function=embeddings_model,\n",
    "            )\n",
    "            print(\"FAISS vector store loaded successfully from MongoDB.\")\n",
    "            return vectorstore\n",
    "\n",
    "        else:\n",
    "            print(\"No embeddings found in MongoDB. Creating new FAISS index...\")\n",
    "\n",
    "            # Step 5: Create FAISS index dynamically based on the index type\n",
    "            if not text_chunks:\n",
    "                print(\"No text chunks available to create embeddings.\")\n",
    "                return None\n",
    "\n",
    "            first_chunk = text_chunks[0]\n",
    "            sample_embedding = embeddings_model.embed_query(first_chunk)\n",
    "            embedding_size = len(sample_embedding)\n",
    "            print(f\"Determined embedding size: {embedding_size}\")\n",
    "\n",
    "            # Create the appropriate FAISS index based on the specified index type\n",
    "            if index_type == \"Flat\":\n",
    "                index = faiss.IndexFlatL2(embedding_size)\n",
    "            elif index_type == \"IVF\":\n",
    "                nlist = 100\n",
    "                quantizer = faiss.IndexFlatL2(embedding_size)\n",
    "                index = faiss.IndexIVFFlat(quantizer, embedding_size, nlist)\n",
    "            elif index_type == \"HNSW\":\n",
    "                index = faiss.IndexHNSWFlat(embedding_size, 32)\n",
    "            print(index)\n",
    "\n",
    "            # Step 6: Convert text to embeddings and add to index\n",
    "            print(\"Converting text to embeddings and adding to index...\")\n",
    "            vectorstore = FAISS.from_texts(\n",
    "                texts=text_chunks, embedding=embeddings_model\n",
    "            )\n",
    "\n",
    "            # Step 7: Serialize and compress FAISS index and docstore\n",
    "            faiss_index_binary = compress_data(pickle.dumps(vectorstore.index))\n",
    "            docstore_binary = compress_data(pickle.dumps(vectorstore.docstore))\n",
    "            index_to_docstore_id_binary = compress_data(\n",
    "                pickle.dumps(vectorstore.index_to_docstore_id)\n",
    "            )\n",
    "\n",
    "            # Step 8: Store serialized and compressed data in MongoDB\n",
    "            embeddings = {\n",
    "                \"faiss_index\": faiss_index_binary,\n",
    "                \"docstore\": docstore_binary,\n",
    "                \"index_to_docstore_id\": index_to_docstore_id_binary,\n",
    "            }\n",
    "            collection.insert_one(embeddings)\n",
    "            print(\"FAISS vector store created and saved to MongoDB.\")\n",
    "            return vectorstore\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(\"Error in create_or_load_vectorstore:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Save Embeddings Endpoint (To be Tested in Flask Environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@app.route(\"/save-embeddings\", methods=[\"POST\"])\n",
    "async def save_embeddings():\n",
    "    \"\"\"Route for saving embeddings from files.\"\"\"\n",
    "    file_folder_path = \"pdfs\"  # Path to your folder containing PDF files\n",
    "\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(file_folder_path):\n",
    "        return jsonify({\"error\": \"Folder not found.\"}), 404\n",
    "\n",
    "    # Get all PDF files in the folder\n",
    "    pdf_files = [\n",
    "        os.path.join(file_folder_path, f)\n",
    "        for f in os.listdir(file_folder_path)\n",
    "        if f.endswith(\".pdf\")\n",
    "    ]\n",
    "\n",
    "    # Ensure there are PDF files in the directory\n",
    "    if not pdf_files:\n",
    "        return jsonify({\"error\": \"No supported files found in the directory.\"}), 400\n",
    "\n",
    "    # Extract text from all PDF files\n",
    "    extracted_text = extract_text_from_files(pdf_files)\n",
    "    if not extracted_text:\n",
    "        return jsonify({\"error\": \"No text extracted from files.\"}), 400\n",
    "\n",
    "    # Join the extracted text into a single string\n",
    "    combined_text = \"\\n\".join(extracted_text)\n",
    "\n",
    "    # Split the combined text into chunks\n",
    "    text_chunks = split_text_into_chunks(combined_text)\n",
    "\n",
    "    # Continue with creating/loading the vector store\n",
    "    vectorstore = await create_or_load_vectorstore(text_chunks, index_type=\"HNSW\")\n",
    "\n",
    "    if vectorstore is None:\n",
    "        return jsonify({\"error\": \"Failed to create or load vector store.\"}), 500\n",
    "\n",
    "    return jsonify({\"message\": \"Embeddings saved successfully in MongoDB!\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Chat Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route(\"/chat\", methods=[\"POST\"])\n",
    "async def chat():\n",
    "    \"\"\"Handle user questions and retrieve responses.\"\"\"\n",
    "    data = request.get_json()\n",
    "    user_question = data.get(\"user_question\")\n",
    "    user_id = \"1\"\n",
    "    if not user_question:\n",
    "        return jsonify({\"error\": \"Missing user_question parameter.\"}), 400\n",
    "\n",
    "    # Load existing embeddings\n",
    "    vectorstore = await create_or_load_vectorstore([], index_type=\"HNSW\")\n",
    "\n",
    "    if vectorstore is None:\n",
    "        return jsonify({\"error\": \"Failed to create or load vector store.\"}), 500\n",
    "\n",
    "    prompt = prompt_template.format(user_question=user_question)\n",
    "    retrieval_chain = create_retrieval_chain_with_ollama(vectorstore)\n",
    "\n",
    "    response = retrieval_chain.invoke({\"question\": prompt})\n",
    "    answer = response.get(\"answer\", \"Sorry, I couldn't find an answer.\")\n",
    "\n",
    "    # Save chat history in MongoDB\n",
    "    chat_history_entry = {\n",
    "        \"user_id\": user_id,\n",
    "        \"timestamp\": datetime.now(timezone.utc),\n",
    "        \"user_question\": user_question,\n",
    "        \"bot_answer\": answer,\n",
    "    }\n",
    "    chat_history_collection.insert_one(chat_history_entry)\n",
    "\n",
    "    return jsonify({\"answer\": answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
